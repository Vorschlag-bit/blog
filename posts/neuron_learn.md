---
title: "신경망은 어떻게 '학습'할까?"
date: "2026-01-08 22:09:23"
category: "딥러닝"
description: "신경망이 자동으로 학습하는 과정에 대해서 알아보자"
---

## 신경망 학습
'학습'이란 <b>훈련 데이터로부터 가중치 매개변수의 최적값을 자동으로 획득</b>하는 걸 의미한다.
이를 가능하게 하는 것은 지표인 손실 함수이다. '학습'의 목표는 손실 함수의 결괏값을 가장 작게 만드는 가중치 매개변수를 찾는 것이다.

손실 함수의 값을 가급적 작게 만드는 기법으로, 함수의 기울기를 활용하는 경사법이 있다.

## 데이터로부터 학습
신경망의 특징은 데이터를 보고 학습할 수 있다는 점이다. 데이터에서 학습한다는 건 가중치 매개변수의 값을 데이터를 보고 자동으로 결정한다는 것이다.

만약 모든 매개변수를 수작업으로 결정한다면 딥러닝은 존재하지 않았을 것이다. 당장 퍼셉트론에서도 매개변수의 값은 사람이 직접 설정했었다, 허나 이때는 매개변수가 고작 3개였지만 실제 신경망에서 매개변수는 수천에서 수만 개가 존재한다.

거기서 층을 더 깊게 한다면 억에서 조로 넘어갈지도 모른다. 이런 어마어마하게 큰 수를 수작업으로 하는 것은 불가능하다. 이 글에서는 신경망 학습(데이터로부터 매개변수의 값을 정하는 법)에 대해서 학습하고 파이썬으로 MNIST 데이터셋의 손글씨 숫자를 학습하는 코드를 구현한다.

### 데이터 주도 학습
머신러닝에서 가장 중요한 것은 데이터이다. 데이터로부터 답을 찾고, 패턴을 발견하는 과정 자체가 머신러닝이기 때문이다.

사람과 머신러닝은 이 점이 문제해결에 있어서 가장 큰 차이점이라고 볼 수 있다.  
사람은 경험과 직관을 바탕으로 시행착오를 거쳐 작업을 진행한다. 하지만 머신러닝은 사람의 개입을
최소화하고 수집한 데이터로부터 패턴을 찾으려고 시도한다. 또한 신경망과 딥러닝은 기존 머신러닝에서 사용하던 방법보다 사람의 개입을 더욱 배재하는 특성을 갖고 있다.

만약 손글씨를 인식하는 프로그램을 구현한다고 했을 때, 명확한 규칙성이 없기 때문에 구현의 난이도가 매우 어렵다는 걸 알 수 있다. 따라서 알고리즘을 구체적으로 설계하기 보단 차라리
주어진 데이터를 바탕으로 특징을 학습하는 게 나을 거라는 생각이 들 것이다.

여기서 말하는 특징은 입력 데이터(입력 이미지)에서 본질적인 데이터를 정확하게 추출할 수 있도록 설계된 변환기를 말하낟. 이런 특징을 사용해 이미지 데이터를 벡터로 변환하고, 변환된 벡터를 갖고 지도 학습 방식의 대표 분류 기법인 SVM,KNN 등으로 학습할 수 있다.

이처럼 머신러닝에서 모아진 데이터로부터 규칙을 찾아내는 역할을 기계가 수행한다. 다만 이미지를 벡터로 변환할 때 사용할 특징은 여전히 사람이 선택해야 한다는 점에 주의하자.

이 말은 문제에 적합한 특징을 쓰지 않으면 훌륭한 결과를 얻을 수 없다는 것이다. 즉, 특징과 머신러닝을 활용한 접근에도 '사람'이 적절한 특징을 생각해내야 한다.

반면 신경망(딥러닝)은 사람이 일절 개입하지 않는다. 신경망은 이미지를 '있는 그대로' 학습한다. 특징과 머신러닝까지는 특징에 대해 사람이 설계를 했으나, 신경망은 이미지를 포함한 특징까지도 기계가 스스로 학습한다.

이러한 점으로 딥러닝을 종단간 머신러닝이라고도 한다. 여기서 종단간은 '처음부터 끝까지'라는 의미로 데이터에서 목표한 결과를 사람의 개입 없이 얻는다는 뜻이다.

신경망의 이점은 모든 문제를 같은 맥락에서 풀 수 있다는 점이다. 예를 들어 '5'를 인식하는 문제든, '햄버거'를 인식하는 문제든 어떠한 세부사항과 관계없이 신경망은 데이터를 온전히 학습하고 주어진 문제의 '패턴'을 발견하려고 시도한다.

즉, 신경망은 모든 문제를 주어진 데이터 그대로를 입력 데이터로 활용해 'end to end'로 학습할 수 있다.

### 훈련 데이터와 시험 데이터
머신러닝에서 데이터를 취급할 때 주의할 점이 있다. 머신러닝 문제는 데이터를 '훈련 데이터'와 '시험 데이터'로 나눠 학습과 실험을 수행하는 것이 일반적이다.

우선 훈련 데이터만 사용해 최적의 매개변수를 찾는다. 그런 다음 시험 데이터를 사용해 앞서 훈련한 모델의 실력을 평가한다.

왜 훈련 데이터와 시험 데이터를 분리해야 할까? 우리가 원하는 건 범용성을 갖춘 모델이기 때문이다. 이 범용성을 제대로 평가하기 위해선 훈련 데이터와 시험 데이터를 분리해야 한다.

범용성은 아직 훈련하지 않은 데이터로도 문제를 올바르게 풀어낼 수 있는 능력이다. 머신러닝의 최종 목표는 곧 범용성 획득이다.

만약 수중에 있는 훈련 데이터만 잘 판별한다면 그 데이터만 학습했을 가능성이 크다. 따라서 데이터셋 하나로만 매개변수의 학습과 평가를 수행하면 올바른평가가 될 수 없다. 수중 데이터셋은 제대로 맞히더라도 다른 데이터셋은 엉망일 수 있다.

참고로 데이터셋에 지나치게 최적화된 상태를 <b>과대적합(overfitting)</b>이라고 한다.

## 손실 함수
신경망 학습에선 현재 상태를 '하나의 지표'로 표현한다. 그리고 그 지표를 가장 좋게 만들어주는
가중치 매개변수의 값을 탐색한다. 신경망 학습에서 사용하는 이 '하나의 지표'는 <b>손실 함수(loss function)</b>라고 한다.

이 손실 함수는 임의의 함수를 사용할 수 있으나 일반적으로는 오차제곱합과 교차 엔트로피 오차를 사용한다.

손실 함수는 신경망 성능의 '나쁨'을 나타내는 지표로, 현재 신경망이 훈련 데이터를 얼마나 잘 처리하지 못하는가를 나타낸다. 성능상 나쁨이 적은 것은 좋다는 의미이므로 수행에 있어서 문제는 없다.

### 오차제곱합
가장 많이 쓰이는 손실 함수는 오차제곱합(sum of squares for error)이다. 수식으로는 아래와 같다.
$$
E = \frac{1}{2} \sum_{k} (y_k - t_k)^2
$$
여기서 $y_k$는 신경망의 출력(신경망이 추정한 값), $t_k$는 정답 레이블, $k$는 데이터의 차원 수를 의미한다.

오차제곱합은 각 원소의 출력(추정 값)과 정답 레이블(참 값)의 차($y_k-t_k$)를 제곱한 후에 그 총합을 구한다. python으로 구현하면 아래와 같다.

```python
def sum_squares_error(y,t):
    return 0.5 * np.sum((y-t)**2)
```

이 함수를 사용해서 실제 데이터에 대한 학습하는 간단한 예시를 보자.

```python
# 정답은 2(one-hot encoding) 정답 레이블
t = [0,0,1,0,0,0,0,0,0,0]
# 2가 제일 높을 거라고 추정한 추정값 배열
y = [0.1,0.05,0.6,0.0,0.05,0.1,0.0,0.0,0.0]
sum_squares_error(np.array(y),np.array(t))
# return == 0.0975....
```

오차제곱합 기준으로는 <b>오차가 더 작은</b> 것이 정답에 더 가깝다고 판단할 수 있다.

### 교차 엔트로피 오차
또 다른 손실함수로서 <b>교차 엔트로피 오차(cross entroy error, CEE)</b>도 자주 사용된다.

교차 엔트포리 오차의 수식은 아래와 같다.

$$
E = -\sum_{k} t_k  \log y_k
$$

여기서 log는 밑이 $e$인 자연 로그이고, $y_k$는 신경망의 출력, $t_k$는 정답 레이블이다.

또한 $t_k$는 정답에 해당하는 인덱스만 1이고 나머지는 0인 원-핫 인코딩으로 되어 있다. 따라서 위의 식은
실질적으로 정답일 때의 추정($t_k$가 1일 때의 $y_k$)의 자연로그를 계산하는 식이 된다.

예를 들어 정답 레이블이 2가 정답이고, 이때의 신경망 출력이 0.6이면 교차 엔트로피 오차는
$-log0.6=0.51$이다. 또한 같은 조건에서 신경망 출력이 0.1이면 $-log0.1=2.30$이다.

즉, 교차 엔트로피 오차는 정답일 때의 출력이 전체 값을 정하게 된다.

<figure>
    <img src="/images/log.png" alt="자연로그 y=logx 그래프" />
    <figcaption>자연로그 y = logx의 그래프</figcaption>
</figure>

위의 그래프에서 볼 수 있듯이 $x$가 1일 때, $y$는 0이 되고, $x$가 0에 가까워질수록 $y$의 값은 점점 작아진다. 따라서 교차 엔트로피 오차도 정답에 해당하는 출력이 커질수록 0에 다가가고, 그 출력이 1일 때(최대일 때) 0이 된다. 반대로 정답일 때의 출력이 작아지면 오차가 커진다.

교차 엔트로피를 python으로 구현하면 아래와 같다.
```python
def cross_entroy_error(y,t):
    delta = 1e-7
    return -np.sum(t * np.log(y+delta))
```

여기서 $y$와 $t$는 넘파이 배열이다. 이 코드에서 주의깊게 볼 것은 `delta`라는 변수이다. 이 변수는
`np.log()` 함수에 $0$을 입력할 경우 음의 무한대를 뜻하는 `-inf`가 되어서 오버 플로우로 인해 계산을 수행할 수 없기 때문에 더하는 값이다.

즉, 아주 작은 값을 더해서 절대 0이 되지 않도록 조정하는 값이다.

### 미니배치 학습 (손실 함수 오차)
머신러닝 문제는 훈련 데이터를 사용해 학습한다. 좀 더 구체적으로 말하면 훈련 데이터에 대한 손실 함수의 값을 구하고, 그 값을 최대한 줄여주는 매개변수를 찾아낸다. 이렇게 하려면 모든 훈련 데이터를 대상으로 손실 함수 값을 구해야 한다. 즉, 훈련 데이터가 100개라면 그로부터 계산한 <b>100개의 손실 함수 값의 합</b>을 지표로 삼는 것이다.

만약 학습할 데이터가 N가 있다면 어떻게 될까? 교차 엔트로피 식은 아래와 같이 바뀌게 될 것이다.

$$
E = -\frac{1}{N} \sum_{n} \sum_{k} t{_n}{_k}  \log y_k
$$

이때 데이터가 $N$개라면 $t{_n}{_k}$는 $n$번째 데이터의 $k$번째 값을 의미한다.
이전의 데이터 1개에 대한 손실 함수를 N개로 확장하고, 마지막에 $N$으로 나누어 정규화를 한 것이다.

$N$으로 나눔으로써 '평균 손실 함수'를 구하는 것이다. 문제는 훈련 데이터의 개수가 빅데이터 수준이 된다면 수백만에서 수천만도 넘는 수일 것이다. 이 많은 데이터를 대상으로 일일이 손실 함수를 계산하는 것은 불가능하다.

이런 경우 데이터의 일부를 추려 전체의 <b>근사치</b>로 이용한다. 신경망 학습에서도 훈련 데이터로부터
일부만 골라 학습을 수행ㅎ나다. 이를 <b>미니배치(mini batch)</b>라고 한다.

수 만개의 훈련 데이터가 있더라도 그 중 100단위 개수만 무작위로 뽑은 후 그 데이터만 사용해 학습을 하는 것이다. 이러한 학습 방법을 미니배치 학습이라고 한다.

만약 훈련 데이터가 60000개, 정답 레이블이 10줄인 데이터에서 무작위로 10개를 뽑는다면 함수를 어떻게 구현할 수 있을까?

`np.random.choice()` 함수를 쓰면 아래와 같이 쉽게 해결할 수 있다.

```python
tran_size = x_train.shape[0]
batch_size = 10
batch_mask = np.randowm.choice(train_size, batch_size)
x_batch = x_train[batch_mask]
t_batch = t_train[batch_mask]
```

`np.random.choice()` 함수는 지정한 범위의 수 중 무작위로 원하는 개수만 꺼낼 수 있다.

### 미니배치 학습 (교차 엔트로피 오차)
교차 엔트로피 오차는 미니배치 같은 배치 데이터를 어떻게 구현할 수 있을까?  앞서 구현한 데이터 1개를 처리하는 교차 엔트로피 오차 함수를 약간만 수정하면 된다.

```python
def cross_entropy_error(y,t):
    if y.ndim == 1:
        t = t.reshape(1, t.size)
        y = y.reshape(1, y.size)
    batch_size = y.shape[0]
    return -np.sum(t*np.log(y+le-7)) / batch_size
```

이 코드에서 $y$는 신경망의 출력, $t$는 정답 레이블이다. $y$가 1차원이라면 `reshape` 함수로 데이터 형상을 바꿔서 `batch_size` 변수에 대한 일관성을 보장했다.

그리고 이 배치 크기로 나눠 정규화를 하고 이미지 1장단 평균 교차 엔트로피 오차를 계산하면 된다.

### 손실 함수 존재의 의의
손실 함수 정의는 대강 이해할 수 있으나, 왜 굳이 손실 함수를 사용해야 하는가는 여전히 의문이다.

숫자 인식이라는 문제도 궁극적 목표인 높은 정확도를 유도하는 매개변수를 찾는 것이다. 이렇게 <b>'정확도'</b>라는 지표를 두고, <b>'손실 함수의 값'</b>이라는 우회적 방법을 택하는 이유는 뭘까?

이는 <b>'미분'</b> 때문이다. 신경망 학습에서는 최적의 매개변수(가중치와 편향)를 탐색 시, 손실 함수의 값을 가능한 한 작게 하는 매개변수 값을 찾는다. 이때 매개변수의 미분(정확히는 순간 기울기)을
계산하고, 그 미분 값을 단서로 매개변수의 값을 서서히 갱신하는 과정을 반복한다.

가중치 매개변수의 손실 함수 미분이라 '가중치 매개변수의 값을 아주 약간 변화시킬 때, 손실 함수의 변화량'이다. 만약 이 변화량이 음수라면 가중치 매개변수를 양의 방향으로 변화시켜 손실 함수의 값을 줄일 수 있다. 반대로 변화량이 양수면 매개변수를 음의 방향으로 변화시켜 손실 함수의 값을 줄일 수 있다.

그러나 변화량이 0이면 가중치 매개변수를 어디로 움직여도 손실 함수의 값은 변화할 수가 없다. 따라서 가중치 매개변수의 갱신은 멈추게 된다.

정확도를 지표로 삼아서 안 되는 이유는 미분 값이 대대분의 장소에서 0이 되어 매개변수를 갱신할 수 없기 때문이다.

정확도 매개변수의 아주 작은 변화에는 거의 반응을 보이지 않고, 반응이 있더라도 그 값이 불연속적으로 갑작스럽게 변화한다. 이는 '계단 함수'를 활성화 함수로 사용하지 않는 이유와 같다.

만약 활성화 함수로 계단 함수를 사용하면 값이 불연속적으로 갑자기 변화하기 때문에 신경망 학습이 잘 이뤄지지 않는다. 계단 함수의 기울기는 대부분 0이다. 즉 매개변수의 작은 변화를 대부분 묵살해 손실 함수의 값에 아무런 변화를 주지 않는다.

반면 시그모이드 함수의 접선의 기울기는 <b>연속적으로 변한다</b>. 즉, 시그모이드 함수의 기울기는
결코 0이 되지 않는다. 이 성질 덕분에 신경망은 올바르게 학습할 수 있다.

## 수치 미분
경사법에서는 기울기 값을 기준으로 나아갈 방향을 결정한다. 기울기의 정의와 어떠한 성질이 있는지
고등학교 수학을 복습해보자.

### 미분
미분은 '특정 순간'의 변화량을 뜻한다. 만약 달리기를 통해 10분간의 평균 속도를 구할 수 있을 것이다.
이는 10분간의 거리를 10분으로 나누면 구해진다. 1분간의 평균 속도도 같은 원리로 구할 수 있다.

그렇다면 이 간격을 최대한 줄여서 0에 가깝게 하면 어떻게 될까? 즉, 한순간의 변화량을 구하는 건 어떻게 될까? 이게 곧 미분이다. 한순간의 변화량이며, 수식적으로 아래와 같다.

$$
frac{df(x)}{dx} = \lim_{h \to 0} \frac{f(x+h) - f(x)}{h}
$$

좌변은 $f(x)$의 $x$에 대한 미분($x$에 대한 $f(x)$의 변화량)을 나타내는 기호이다.  
결국 $x$의 작은 변화가 함수 $f(x)$를 얼마나 변화시키느냐를 의미한다. 이때 시간의 작은 변화, 즉
시간을 뜻하는 $h$는 0에 수렴하는 아주 작은 값이다.

미분을 python으로 구현하면 $0$에 수렴하는 아주 작은 값을 표현하기 위해 아래와 같이 구현할 수 있다.
```python
def diff(f,x):
    h = 1e-50
    return (f(x+h) - f(x)) / h
```
위 함수는 그럴한 미분을 구현한 것처럼 보인다. 앞서 사용한 `1e-50`은 소수점 이하 50자릿수라는 의미이므로 아주 아주 작은 값이다. 허나 이 방식은 <b>반올림 오차(rounding error)</b> 문제를 일으킬 수 있다.

반올림 오차는 작은 값이 생략되어 최종 계산 결과에 오차가 생기게 한다. 당장 python으로 아래와 같은 식을 넣으면 0.0을 return 한다.

```python
np.float32(1e-50)
# 0.0
```
1e-50을 float32(32비트 부동소수점)으로 나타내면 0.0이 되어 올바르게 표현할 수 없다. 따라서 너무 작은 값은 너무 큰 값을 사용하는 것과 마찬가지로 컴퓨터로 계산하는 데에 쉽지 않다.

따라서 $h$를 $10^-4$ 정도로 사용하는 것이 일반적이다.

또한, 이 함수는 $f$의 차분과 관련된 개선이 필요하다. 현재 구현은 $x_h$와 $x$ 사이의 함수 $f$의 차분을 계산하고 있으나 애당초 이 계산에는 오차가 있다.

'미분은' $x$ 위치의 함수 기울기에 해당하나, 이 구현은 단순히 두 점 사이의 기울기를 구하는 것이기 때문이다. 그래서 진정한 미분(접선의 기울기)과 엄밀히 일치하지 않는다. (전방 차분의 한계)

이는 $h$를 0에 수렴하는 것이 불가능해 생기는 한계이다.

이 오차를 줄이기 위해서 $(x+h)$와 $(x-h)$일 때의 함수 $f$의 차분을 계산하는 방법을 쓰기도 한다.
이 차분은 $x$를 중심으로 그 전후의 차분을 계산한다는 의미에서 중심 차분 혹은 중앙 차분이라 한다.

이 두 개선점을 적용한 함수는 아래와 같다.
```python
def diff(f,x):
    h = 1e-4
    return (f(x+h) - f(x-h)) / (2*h)
```
중앙 차분을 사용하는 이유는 예를 들어 $y=x*2$의 2차 함수 그래프를 생각해보면 된다.

2차 함수 그래프의 특징은 $x$가 커질수록 그래프의 기울기가 훨씬 커진다는 것이다. 따라서 앞서 사용한 전방 차분을 생각해보면, 순수하게 자신 앞의 값에 대한 기울기이므로 커지는 데에 반해
중앙 차분은 그보다 더 완만한 기울기를 갖게 될 것이다.

이해가 안 된다면 단순히 $x$가 0일 때 $x$와 $h$ 사이의 기울기보다 $x-h$와 $x+h$ 사이의 기울기가 훨씬 작다는 걸 보면 쉽게 이해될 것이다.

