---
title: "AI를 활용한 새로운 프로젝트를 해보자 (with Fast API)"
date: "2026-02-04 13:12:40"
category: "개발"
description: "AI를 활용한 데이터 파이프 라인을 구축해보는 프로젝트를 시작해보자."
---

## 서론: 뭔갈 새로운 걸 해보고 싶어요
블로그 개발이 끝나고 다시 공고를 지원해보다가, 면접 중 한 현직 개발자 분께서 $I$형 인재보다 $T$자형 인재가 되길 추천한다고 말씀하셨다.

개발자로서 AI를 매일 사용하면서, AI의 개발 실력을 보면 누구나 다 감탄할 것이다.

하지만 이 감탄 뒤에는 언제나 걱정이 뒤따라 온다. 웬만한 경력 개발자 이상의 실력을 갖춘 AI를 경력 개발자가 이용한다면,
같은 경력 개발자끼리의 경쟁도 힘겨운 마당에 나 같은 신입에겐 이는 사형선고와 다름이 없을 것이다.

그 분께선 결론적으로 한 가지 능력을 깊게 파는 것보다 여러 능력을 조금 얕더라도 두루 갖춘 개발자일수록 회사에선 더 매력적으로 생각할 수밖에
없다고 말씀하셨다. 이젠 한 가지 발차기를 만 번하는 것보다 여러 발차기를 1000번 연습하는 게 나은 시대가 된 셈이다..

하지만 새로운 걸 학습하는 건 언제나 즐거운 일이고, 단순히 취미를 떠나서 이 학습 내용이 나의 시야와 실력을 폭넓게 향상시켜 줄 것이라는 믿음은
절대적이므로 나 역시 새로운 프로젝트를 완전히 새로운 기술 스택을 사용해 개발해보기로 결심했다.

## 뭘 써보지?
사실 주제를 먼저 결정하는 것이 우선이겠지만, 내 프로젝트는 개발자로서의 의의가 더 강하기 때문에 어떠한 기술을 써보느냐가 더 중요했다.

내가 이번 프로젝트에서 사용해보고 싶었던 건 크게 2가지였다.

### 1. Fast API
Python 기반 서버를 운영해보고 싶었어. 요즘 중소/중견 기업에서 Fast API 개발자를 찾는 걸 정말 자주 볼 수 있다. 
Fast API 코드를 볼 때마다 그렇게 복잡하진 않다고 느꼈고, 평소 코테 언어가 python이다보니 익듁하게 사용할 수 있을 거라 판단해서 써보고 싶었다.

### 2. 크롤링 봇 + AI를 활용한 데이터 전처리
특정 주제에 맞는 데이터를 크롤링해서 내가 정의한 스키마에 맞게 전처리한 후 저장하는 파이프 라인 구축이 궁극적 목표다보니 이 부분도 매우 중요했다.  
둘 다 당연히 처음하는 것이고, AI를 활용해서 단순히 프롬프트 기반으로 데이터 전처리가 그렇게 효율적으로 이뤄질까가 매우 궁금했었다.

## 뭘 만들지?
이제 주제를 정할 차례였다. 크게 2가지 후보군이 있었다.

### 1. 영화 관련 사이트
영화 리뷰 감상 분석 및 AI 큐레이션을 제공해주는 사이트를 만드는 것이다. 왓챠피디아나 네이버 영화, IMDB 같은 영화 리뷰 사이트를 대상으로 데이터를 크롤링한 후
AI에게 맡겨 데이터를 내가 정의한 스키마에 맞게 저장하고 박스 오피스의 AI 긍정 지수 순위를 제공.

### 2. 주식 관련 사이트
주가 데이터를 크롤링해서 데이터를 분석한 후에 AI 전망과 통계를 제공해주는 사이트. 

몇 가지 이유때문에 '영화 관련 사이트'를 만들기로 결정했다.

1. 비정형화된 데이터
LLM은 주가 정보처럼 숫자 계산보단 <b>텍스트의 맥락 이해</b>에 훨씬 강력하다.
2. 정보 보안
주가 정보에 대한 데이터 크롤링은 보안이 까다롭고, 잘못된 정보 제공에 따른 리스크가 너무 크다. 

이러한 결론을 바탕으로 단순한 영화의 평점이 아니라 그 너머에 있는 상세한 리뷰를 바탕으로 <b>'현재 대중의 감정과 분위기'</b>를 분석해 영화를 추천해주는 서비스를 구현하기로 결정했다.

## 프로젝트 구조 설계
우선 프로젝트 구조에 대한 간단한 설계를 했다.

당연히 가장 중요한 건 가격이다. 완전한 무료를 꿈꾸는 내게 최적화된 아키텍쳐여야 했다. 우선 3티어 아키텍쳐를 바탕으로 제공되는 서비스가 내겐 가장 익숙하고,
Fast API를 돌릴 서버가 당연히 존재해야 하므로 3티어 아키텍쳐를 선택했다.

FE는 내게 가장 익숙한 현재 사용 중인 Vercel + Next.js 조합을 사용하고, BE는 Render를 사용해서 무료 티어에서 서버를 돌리기로 결정했다. 다만 사용자가 없으면 잠드므로 주기적으로 ping을 날리는 daemon을 추가적으로 만들면 좋을 거 같았다.

마지막으로 DB는 무료 DB로 유명한 Superbase를 사용하기로 했다. Postgresql은 이전에도 사용해보았기 때문에 익숙하고, Postgresql이 제공해주는 JSON 데이터 저장 기능 또한 내가 제공하는 <b>비정형 데이터</b>에 매우 최적화되어 있으므로 아주 적절하다.

### 데이터 스키마를 어떻게 정의할까?
이렇게 구조까지 대강 결정하고 나니 BE 개발 방식대로 DB 설계부터 하고 싶었다. 하지만 가장 큰 문제가 있었다.

데이터가 어떤 식으로 크롤링되고 어떻게 전처리되어 제공될지 전혀 모르는 상황에선 DB를 설계하는 건 어불성설이다. 이에 따라 크롤링 봇과
LLM 기능부터 가볍게 구현을 해봐야 했었다.

AI 프로젝트에선 <b>ELT(Extract, Load, Transform)</b> 방식이 유용하다. 일단 긁어서 저장하고(Extract, Load), 나중에 전처리(Transform)하는 것이 편하다는 의미이다. 따라서 테이블을 2개로 나눴다.

1. <b>Raw Table</b>: 크롤링한 날 것의 데이터를 저장할 테이블, 스키마 정의를 지키기 위해서 JSON으로 저장할 예정
```SQL
CREATE TABLE raw_reviews (
    id SERIAL PRIMARY KEY,
    movie_title VARCHAR(255),
    crawl_date TIMESTAMP,
    raw_content JSONB -- 여기에 크롤링한 딕셔너리 통째로 저장
);
```

2. <b>Processed Table</b>: 정형화된 데이터를 저장할 테이블, 정규화된 스키마이며 LLM에게 이 스키마에 맞춰 데이터를 제공하라고 말할 것이다.

```SQL
CREATE TABLE movie_insights (
    id SERIAL PRIMARY KEY,
    movie_title VARCHAR(255),
    summary TEXT,           -- AI가 요약한 3줄
    sentiment_score INT,    -- AI가 분석한 점수 (0~100)
    keywords TEXT[],        -- AI가 추출한 태그 배열 (Postgres 배열 타입 지원)
    updated_at TIMESTAMP
);
```

재밌는 점은 크롤링할 데이터의 형태를 전혀 몰라도 Processed Table을 정의할 수 있었다는 점이다. 어차피 LLM이 전처리를 이에 맞춰 해줄 것이기 때문이다.

<figure>
    <img src="/images/superbase1.png" alt="superbase 화면 모습" />
    <figcaption>Superbase 화면 모습, SQL Editor를 사용해 SQL로 쉽게 테이블을 생성할 수 있다.</figcaption>
</figure>

<figure>
    <img src="/images/sbase1.png" alt="superbase 화면 모습" />
    <figcaption>Superbase 테이블 화면 모습, 테이블 생성된 걸 확인할 수 있고 데이터 row와 보안 수준까지 설정이 가능하다.</figcaption>
</figure>

<code>movie_insights</code> 테이블에 대한 보안 설정은 아래 사진처럼 해놓았다. <b>Permissive</b>로 두면, 하나라도 통과하면 pass하는 OR 조건문으로 동작하고, <b>Restrictive</b>로 두면, 모든 정책을 통과해야만 pass가 되는 AND 조건문이다.

이 테이블은 단순히 내 페이지를 방문하는 사람들이 볼 데이터이므로, 누구나 볼 수 있고(<code>public</code>), select만 가능하도록 하기 위해 <code>SELECT</code>만 설정하고 <code>true</code>를 통해 select 이외에는 모두 차단했다.

<figure>
    <img src="/images/sbase3.png" alt="superbase 화면 모습" />
    <figcaption>Superbase 테이블 화면 모습, 테이블 생성된 걸 확인할 수 있고 데이터 row와 보안 수준까지 설정이 가능하다.</figcaption>
</figure>

<code>raw_reviews</code> 테이블은 RLS만 설정해두고 별다른 정책을 만들지 않았다. 이렇게 하면, 기본적으로 <code>DENY ALL</code>로 되어 외부에선 접근하지 못 한다. Fast API 서버에선 <code>service_role</code> key를 사용해서, DB에 접근할 것이기 때문에 이 키를 사용하지 않은 외부 접근은 다 차단하면 된다.

