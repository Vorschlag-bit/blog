---
title: "신경망은 뭐지?"
date: "2026-01-01 19:20:18"
category: "딥러닝"
description: "퍼셉트론과 신경망의 차이점에 대해서 알아보고 신경망의 특징에 대해서 알아보자."
---

## 신경망
이전의 <a href="https://vorschlag-blog.vercel.app/posts/perceptron" class="plink">
  퍼셉트론
</a>을 배울 때 다층 퍼셉트론으로 복잡한 함수를 표현할 수 있다는 걸 알 수 있었다. 하지만 결국 가중치를
설정하는 작업(원하는 결과를 출력하도록 가중치 값을 적절히 정하는 작업)은 여전히 사람이 수동으로 해야 했었다.

하지만 신경망은 이를 해결해준다. 가중치 매개변수의 <b>적절한 값을 데이터로부터 자동으로 학습하는 능력</b>을 갖추고 있기 때문이다.
이 점이 신경망의 가장 큰 특징이다. 신경망이 입력 데이터를 어떻게 식별하는지까지 학습하는 글

### 퍼셉트론 VS 신경망
신경망은 퍼셉트론과 비슷한 점이 많으나 분명히 다른 점도 있다. 신경망을 그림으로 표현하면 아래와 같다.

<figure>
    <img src="/images/neuron.png" alt="신경망 구조의 예시 그림">
    <figcaption>신경망의 예시</figcaption>
</figure>

가장 왼쪽 줄을 입력층, 맨 오른쪽을 출력층이라고 부르며, 중간 줄을 은닉층이라고 한다. 은닉층의 뉴런은 입력층이나 출력층과 달리 사람 눈에
보이지 않는다. (캡슐화) 이 그림만 본다면 퍼셉트론과 큰 차이는 없어 보인다. 뉴런이 연결되는 방식은 퍼셉트론과 동일하기 때문이다.

퍼셉트론은 아래와 같은 구조를 갖추고 있다 (단층 퍼셉트론)
$x_1$과 $x_2$라는 두 신호를 받아서 y를 출력하는 퍼셉트론의 예시이다.

<figure>
    <img src="/images/node_ex.png" alt="입력이 2개인 퍼셉트론 예시 그림">
    <figcaption>입력이 2개인 퍼셉트론 예시</figcaption>
</figure>

이 퍼셉트론을 수식으로 나타내면 아래와 같다.  

$$
y = 
\begin{cases} 
    0\ (b + w_1 x_1 + w_2 x_2 \le 0) \\
    1\ (b + w_1 x_1 + w_2 x_2 > 0)
\end{cases}
$$

여기서 b는 편향으로 뉴런이 얼마나 쉽게 활성화되는지를 제어하고, $w_1,w_2$는 각 신호의 가중치로 각 신호의 영향력을 제어한다.
앞선 그림에선 b가 보이질 않는다. 여기에 편향을 표시하면 아래와 같은 그림이 된다.

<figure>
    <img src="/images/p_2.png" alt="편향을 명시한 퍼셉트론">
    <figcaption>편향을 명시한 퍼셉트론</figcaption>
</figure>

이 퍼셉트론의 동작은 $x_1,x_2,1$이라는 입력에 각각 가중치가 $w_1,w_2,b$로 주어진 상태로 각 신호에 가중치를 곱한 후 다음 뉴런에 전달된다.
다음 뉴런에서는 이 신호들의 합으로 0을 넘으면 1을 출력하고 아니면 0을 출력한다.

위 식을 함수로 나누어서 표현하면 더 간단하게 표현이 가능하다.

$$
y = h(b + w_1x_1 + w_2x_2)
$$

$$
h(x) = 
\begin{cases} 
    0\ (x \le 0) \\
    1\ (x > 0)
\end{cases}
$$

위처럼 바꾸면 입력 신호의 총합이 $h(x)$라는 함수를 거쳐 변환된 후, 그 변환값이 $y$의 출력이 되는 것으로 바뀌었다.
그 후에 $h(x)$ 함수는 입력이 0을 넘으면 1 아니면 0을 return한다.

### 활성화 함수
이렇게 $h(x)$와 같이 <b>입력 신호의 총합을 출력 신호로 변환하는 함수</b>를 활성화 함수라고 부른다.  
즉, 활성화 함수는 입력 신호의 총합이 활성화를 일으키는지 아닌지를 정하는 역할을 수행한다.

위의 식은 가중치가 곱해진 입력 신호의 총합을 계산하고, 그 합을 활성화 함수에 입력해 진짜 결과를 내는 2단계로 처리된다.
따라서 아래와 같이 2개식으로 다시 작성이 가능해진다.

$$
a=b+w_1x_1+w_2x_2
$$

$$
y=h(a)
$$

위의 식은 가중치가 달린 입력 신호와 편향의 총합을 계산한 후, 이를 $a$라고 칭한다. 그리고 $a$를 함수 $h()$에 넣어 $y$를 출력한다.

<figure>
    <img src="/images/fun.png" alt="활성화 함수로 표현한 퍼셉트론의 모습">
    <figcaption>활성화 함수까지 함께 표현한 퍼셉트론 처리 과정</figcaption>
</figure>

위의 그림과 같이 가중치 신호를 조합한 결과가 $a$라는 노드가 되었고, 활성화 함수 $h()$를 통해서 $y$라는 노드로 변환되는 과정으로 표현된다.
뉴런을 그릴 때는 하나의 원으로 그리나 활성화 함수의 기작을 명확하게 보여줄 때는 위의 사진처럼 뉴런 내부의 활성화 처리 과정을 명시하기도 한다.

## 활성화 함수의 종류
$$
h(x) = 
\begin{cases} 
    0\ (x \le 0) \\
    1\ (x > 0)
\end{cases}
$$
위와 같은 활성화 함수는 임겟값을 경계로 출력이 바뀌는데(이 경우 0), 이러한 함수를 <b>계단 함수</b>라고 한다. 따라서 퍼셉트론에서는 활성화 함수로
계단함수를 이용한다고 정의할 수도 있다. 즉, 활성화 함수로 쓸 수 있는 여러 후보 중에서 퍼셉트론은 계단 함수를 채용하고 있는 셈이다.

그렇다면 다른 활성화 함수에는 뭐가 있는 걸까?

### 시그모이드 함수
시그모이드 함수는 신경망에서 자주 이용되는 활성화 함수이다.  
$$
h(x) = \frac{1}{1 + \exp(-x)}
$$

$exp(-x)$는 $e^-x$를 의미하고, $e$는 자연상수로 2.7182의 값을 갖는 실수이다. 구조는 복잡하나 결국 함수 특정 $a$라는 입력이 주어지면
$b$를 return한다. 사실 시그모이드 함수가 중요한 이유는 신경망의 활성화 함수로 사용된다는 점에서 신경망과 퍼셉트론의 차이가 생기기 때문이다.

계단 함수의 경우 <b>경곗값</b>을 바탕으로 값이 달리지는 1차 그래프 같은 성질을 갖고 있다.
```python
def step_f(x):
    if x > 0: return 1
    else return 0
```
물론 이 함수는 x는 실수만 받아들여야 하기 때문에 넘파이 배열을 인수로 넣을 수 없다. 따라서 넘파이 배열을 지원하기 위해선 아래와 같이 수정해야 한다.
```python
def step_f(x):
    y = x > 0
    return y.astype(int)
```
넘파이 배열에 부등호 연산을 수행하면 각 원소에 부등호 연산을 수행한 `bool` 타입 배열이 생성된다.
따라서 y는 `bool` 타입의 배열이 된다. 하지만 계단 함수는 0,1만을 출력하는 `int` 타입 함수이기 때문에 `bool`을 `int`로 바꿔야 한다.
마치 자바스크립트와 같이 보통 컴퓨터에선 0은 `false`, 1은 `true`로 취급된다.

계단 함수로 그래프를 그리면 아래와 같이 표시된다.
```python
def step_f(x):
    return np.array(x > 0, dtype=int)

x = np.arange(-5.0,5.0,0.1)
y = step_f(x)
plt.plot(x,y)
# y축 범위 지정
plt.ylim(-0.1,1.1)
plt.show()
```

<figure>
    <img src="/images/stair.png" alt="계단 함수 그래프의 모습 0을 기점으로 계단 모양">
    <figcaption>사진처럼 0을 경계로 계단 모양을 볼 수 있다.</figcaption>
</figure>

시그모이드 함수는 아래와 같이 간단히 구현이 가능하다.
```python
def sigmoid(x):
    return 1 / (1 + np.exp(-x))
```
이 함수는 넘파이의 브로드캐스트덕분에 넘파이 배열과의 연산이 가능하다. 아까와 동일한 인자로 그래프를 그려보면 아래와 같다.

<figure>
    <img src="/images/sig.png" alt="시그모이드 함수 그래프(S자)의 모습">
    <figcaption>s자(sigmoid) 모양을 확인할 수 있다.</figcaption>
</figure>

### 시그모이드 함수와 계단 함수의 비교
시그모이드 함수와 계단 함수는 곡선과 직선의 차이일 것이다. 계단 함수는 0을 경계로 갑작스러운 출력의 변화가 생기나
시그모이드 함수는 매끄럽게 값이 변화하기 때문이다. 시그모이드의 이 매끈함은 신경망 학습에서 매우 중요한 역할을 수행한다.

계단 함수의 return값이 0과 1인 것에 비해 시그모이드 함수는 다양한 값을 return한다는 점도 매우 다르다. <b>연속적인 실수</b>를 제공한다.

반면 두 함수의 공통점은 결국 입력값에 비례해서 출력값이 증가한다는 점이 같다. 또한 return되는 값의 범위는 결국 0부터 1의 값이다.

## 비선형 함수
계단 함수와 시그모이드 함수의 공통점은 그 밖에도 있다. 둘 다 <b>비선형 함수</b>라는 점이다. 둘 다 1차 방정식의 그래프처럼
하나의 직선으로 이뤄진 것이 아니다.

신경망에서는 활성화 함수로 <b>비선형 함수</b>를 사용해야 한다. 왜 선형 함수는 안 되고 비선형 함수는 되는 걸까?
그 이유는 <b>선형 함수를 사용할 경우 신경망의 층을 깊게하는 의미가 없어지기 떄문</b>이다.

선형 함수의 문제점은 층을 아무리 깊게 해도 '은닉츠이 없는 네트워크'로도 똑같은 기능을 할 수 있다는 점에 있다.  
예를 들어서 $h(x) = cx$라는 선형 함수가 있을 때 이를 3층 네트워크로 만들면 $y(x)=h(h(h(x)))$와 같다. 그리고 이 계산은
$y(x)=c*c*c*x$처럼 곱셉을 3번 수행하나 결국 $y(x)=ax\ a=c^3$으로 표현이 가능하다.

즉, 은닉층이 없는 네트워크로 표한할 수 있다. 이래선 여러 층으로 구성하는 이점을 살릴 수 없다.

### ReLU 함수
시그모이드 함수는 신경망 분야에서 오랜 시간 사용했으나 최근에는 <a href="https://ko.wikipedia.org/wiki/ReLU" class="plink">ReLU</a>
함수를 주로 사용한다. ReLU 함수는 입력이 0을 넘으면 그대로 출력하고 0 이하면 0을 출력하는 함수이다.

$$
h(x) = 
\begin{cases} 
    0\ (x \le 0) \\
    x\ (x > 0)
\end{cases}
$$

수식적으로 위와 같이 표현된다. python 코드로는 `maximum()` 함수를 통해서 간단하게 구현 가능하다.
```python
def relu(x):
    return np.maximum(x,0)
```

## 다차원 배열의 계산
넘파이의 다차원 배열을 사용한 계산법을 숙달하면 신경망을 효율적으로 구현할 수 있다. 다차원 배열도 숫자의 집합이다.
```python
import numpy as np
A = np.array([1, 2, 3z 4])
print(A)
# [1234]
np.ndim(A)
# 1
A.shape
# (4,)
A.shape[ ]
# 4
```
배열의 차원은 `ndim()` 함수를 통해서 알 수 있다. `shape`라는 인스턴스는 배열의 형상을 <b>튜플</b>로 반환한다.
2차원 배열은 <b>행렬</b>이라고 부르며, 가로 방향을 행, 세로 방향을 열이라고 한다.

### 행렬의 곱
행렬의 곱은 왼쪽 행렬의 행과 오른쪽 행렬의 열을 원소별로 곱하고 그 값을 더해서 계산한다.
두 행렬의 곱은 `np.dot()`으로 계산할 수 있다.
```python
A = np.array([[1, 2, 3], [4, 5, 6]])
A.shape
# (2, 3)
B = np.array([[1, 2], [3, 4], [5, 6]])
B.shape
# (3, 2)
np.dot(A, B)
# array([[22, 28],[49, 64]])
```

`np.dot()`은 입력이 1차원 배열이라면 벡터를, 2차원 배열이연 행렬 곱을 계산한다. 여기서 주의할 점은
`np.dot(A,B)`와 `np.dot(B,A)`는 다른 값이 될 수 있다는 점이다. 어쩌면 당연한 것이다. 곱해야할 행과 열이 바뀌었으니
결과가 바뀔 수도 있다.

곱하려는 두 행렬($A,B$)은 반드시 $A$의 열과 $B$의 행이 일치해야 한다. 이걸 <b>행렬의 형상(shape)의 일치</b>라고도 표현한다.

실제로 형상이 일치하지 않는 두 행렬을 곱할 경우 python은 다음과 같은 오류를 출력한다.
```python
C = np.array([[1, 2], [3, 4]])
C.shape
# (2, 2)
A.shape
# (2, 3)
np.dot(A, C)
# Traceback (most recent call last):
# File "<stdin>", line 1, in <module>
# ValueError： shapes (2,3) and (2,2) not aligned: 3 (dim 1) != 2 (dim 0)
```

행렬 연산에서 가장 중요한 2가지는 따라서 방금 말한 행렬의 형상이 일치되어야 한다는 점이다. 다른 한 가지는
결과로 반환되는 행렬의 크기는 이전 두 행렬의 앞 행렬의 행과 뒤 행렬의 열의 크기로 변경된다는 점이다.

$$
A(3,2)*B(2,3)=C(3,3)
$$

### 신경망에서 행렬의 곱
이제 넘파이의 행렬의 사용해서 신경망을 구현해보자.

<img src="/images/n_1.png" alt="신경망 예시 그림">

위와 같은 신경망이 있다고 가정해보자. 이 신경망은 편향과 활성화 함수를 생략하고 가중치만을 갖는다.  
위의 그림을 행렬로 나타내보면 $X$는 $1*2$의 크기를 갖는 행렬, $W$(가중치)는 $2*3$의 크기를 갖는 행렬 그리고
$Y$는 $3*1$의 크기를 갖는 행렬로 정의할 수 있다.

이렇게 행렬의 형상을 반드시 지킨다는 점을 잊어선 안 된다.
위 3개의 행렬에 대한 곱셈을 아래의 코드로 구현할 수 있다.
```python
X = np.array([1,2])
W = np.array([[1,3,5],[2,4,6]])
Y = np.dot(X,W)
print(Y)
# [ 5 11 17]
```

다차원 배열의 스칼라곱을 구해주는 `np.dot()` 함수를 사용하면 이처럼 단번에 결과를 계산한다. 만약 `np.dot()`을 사용하지
않는다면 Y의 원소를 계산해야 하는데 이는 python의 기본 for문이 갖는 오버헤드를 생각하면 매우 무거울 것이다.

따라서 행렬의 곱을 한꺼번에 계산해주는 기능은 신경망을 구현할 때 매우 중요하다.

## 3층 신경망 구현
이번에는 3층 신경망에서 수행되는 입력부터 출력까지의 처리(순방향 처리)를 구현해보자.

<figure>
    <img src="/images/n_2.png" alt="3층 신경망 예시 그림">
    <figcaption>3층 신경망: 입력층(0층)은 2개, 첫 번째 은닉층(1층)은 3개, 두 번째 은닉층(2층)은 2개, 출력층(3층)은 2개의 뉴런으로 구성.</figcaption>
</figure>

다층 신경망에서 가중치를 표기하는 방법에 대해서 간략하게 정리해보았다.
$$
w_{12}^{(1)}
$$
위와 같은 표기가 있을 때, 지수에 있는 $(1)$은 '1층의 가중치'라는 의미로 1층의 뉴런임을 뜻하는 번호이다.  
오른쪽 밑에 있는 두 숫자는 앞은 다음층 뉴런의 뒤는 현재층 뉴런의 인덱스 번호이다. 따라서 $w_{12}^{(1)}$라는 표기의 의미는
앞층의 2번째 뉴런에서 다음 층의 1번째 뉴런으로 향할 때의 가중치라는 의미이다.

<figure>
    <img src="/images/n_3.png" alt="3층 신경망에 편향을 추가한 예시 그림">
    <figcaption>3층 신경망에 편향을 추가한 모습.</figcaption>
</figure>

위의 그림과 같이 편향을 의미하는 뉴런이 추가되었다고 생각해보자. 편향은 오른쪽 아래의 인덱스가 언제나 1로 고정되어 있다는 걸 기억해두자.
편향은 뉴런은 하나뿐이기 때문이다.

앞서 배운 내용을 통해 $a_{1}^{(1)}$을 수식으로 나타낼 수 있다. $a_{1}^{(1)}$은 가중치를 곱한 신호 2개와 편향을 합해서 아래와 같이
계산된다.

$$
a_{1}^{(1)} = w_{11}^{(1)}x_1 + w_{12}^{(1)}x_2 + b_{1}^{(1)}
$$

이를 행렬의 곱을 이용하면 1층의 '가중치 부분을' 아래의 식처럼 간소화할 수 있을 것이다.
$$
A^{(1)} = XW^{(1)} + B^{(1)}
$$
이때 행렬 $A^{(1)}, X,B^{(1)},W^{(1)}$은 다음과 같다.

$$
\mathbf{A}^{(1)} = (a_1^{(1)} \ a_2^{(1)} \ a_3^{(1)}),

\mathbf{X} = (x_1 \ x_2),

\mathbf{B}^{(1)} = (b_1^{(1)} \ b_2^{(1)} \ b_3^{(1)})

\mathbf{W}^{(1)} = 
\begin{pmatrix} 
w_{11}^{(1)} & w_{21}^{(1)} & w_{31}^{(1)} \\ 
w_{12}^{(1)} & w_{22}^{(1)} & w_{32}^{(1)} 
\end{pmatrix}
$$

이를 넘파이를 활용해 구현하면 아래 코드와 같다.
```python
X = np.array([1.0, 0.5])
W1 = np.array([[0.1,0.3,0.5], [0.2,0.4,0.6]])
B1 = np.array([0.1,0.2,0.3])

print(W1.shape)
print(X.shape)
print(B1.shape)

A1 = np.dot(X,W1) + B1
print(A1)
# [0.3 0.7 1.1]
```

여기에 1층의 활성화 함수의 처리를 반영하면 아래의 그림처럼 나타난다.

<figure>
    <img src="/images/n_4.png" alt="3층 신경망을 활성화 함수까지 표현한 예시 그림">
    <figcaption>3층 신경망을 활성화 함수 처리 과정까지 표현한 그림.</figcaption>
</figure>

은닉층에서의 가중치 합(가중 신호와 편향의 총합)을 $a$로 표기하고 활성화 함수 $h()$로 변환된 신호를 $z$로 표기한 그림이다.
여기서 활성화 함수를 시그모이드 함수로 사용하기로 하고 이를 python으로 구현하면 아래와 같다.

```python
def sigmoid(x):
    return 1 / (1 + np.exp(-x))

Z1 = sigmoid(A1)
print(Z1)
# [0.57444252 0.66818777 0.75026011]
```

이어서 1층에서 2층으로 가는 과정과 구현을 살펴보자.

<figure>
    <img src="/images/n_5.png" alt="1층에서 2층 신경망으로 가는 과정을 활성화 함수까지 표현한 예시 그림">
    <figcaption>1층에서 2층으로 신호를 전달하는 과정.</figcaption>
</figure>

```python
W2 = np.array([[0.1,0.4],[0.2,0.5],[0.3,0.6]])
B2 = np.array([0.1,0.2])

A2 = np.dot(Z1,W2) + B2
Z2 = sigmoid(A2)
```

마지막으로 2층에서 출력층으로 신호 전달 역시 마찬가지다. 하지만 마지막 과정에선 활성화 함수만 은닉층과 다르다.
일반적으로 출력 층의 활성화 함수는 $\sigma()$로 표기하며 은닉층의 활성화 함수인 $h()$와는 다르게 표기한다.
($\sigma$는 시그마라고 읽음)

출력층의 활성화 함수는 풀고자 하는 문제의 성질에 맞게 정해야 한다. 예를 들어 회귀에는 항등 함수를, 2클래스 분류에는
시그모이드 함수를, 다중 클래스 분류에는 소프트 맥스 함수를 사용하는 것이 일반적이다.

## 출력층 설계
신경망은 분류와 회귀 모두에 적용이 가능하다. 하지만 어떤 문제에 따라 출력층에서 사용하는 활성화 함수가 달라진다.
일반적으로 회귀에는 항등 함수를, 분류에는 소프트맥스 함수를 사용한다.

머신러닝 문제는 보통 <b>분류(classification)</b>와 <b>회귀(regression)</b>로 나뉜다. 분류는 데이터가 어느 클래스에
속하는지를 판단하는 문제로, 사진 속 인물의 성별따위를 판정하는 문제 같은 것이다. 한편, 회귀는 입력 데이터에서 연속적인 수치를 예측하는 문제로 사진 속
인물의 몸무게를 예측하는 문제가 회귀의 하나라고 볼 수 있다.

### 항등 함수와 소프트맥스 함수
<b>항등 함수는(identity function)</b>는 입력을 그대로 출력하는 함수이다. 입력과 출력이 항상 동일하다는 의미에서 항등이다.
따라서 출력층에서 항등 함수를 사용하면 입력 신호가 곧 출력 신호가 된다.

반면 분류에서 사용하는 <b>소프트맥스 함수(softmax function)</b>는 아래의 식과 같다.
$$
y_k = \frac{\exp(a_k)}{\sum_{i=1}^{n} \exp(a_i)}
$$

$exp(x)$는 $e^x$을 나타내는 지수 함수이다. (e는 자연 상수) n은 출력층의 뉴런 수, $y_k$는 그 중에서 $k$번째 출력임을 나타낸다.
이렇게 소프트맥스 함수의 분자는 입력 신호인 $a_k$의 지수 함수, 분모는 입력 신호의 지수 함수의 합으로 구성된다.

소프트맥스 함수를 그림으로 나타내면 아래의 그림과 같다. 소프트맥스 함수는 모든 입력 신호로부터 화살표를 받는다. 출력층의 각 뉴런이 모든
입력 신호에서 영향을 받기 때문이다.

<figure>
    <img src="/images/n_6.png" alt="소프트맥스 함수를 표현한 그림">
    <figcaption>소프트맥스 함수</figcaption>
</figure>

지수 함수를 사용하는 법은 `np.exp()`라는 함수를 사용하면 쉽게 구할 수 있다.  
이를 통해 소프트맥스 함수를 python으로 구현하면 아래와 같다.
```python
def softmax(x):
    exp_x = np.exp(x)
    sum_exp_x = np.sum(exp_x)
    y = exp_x / sum_exp_x
    return y

```

### 소프트맥스 함수 구현 시 주의사항
앞에서 구현한 `softmax()` 함수는 정확하게 소프트맥스를 구현했으나 컴퓨터로 수를 계산할 때는 <b>오버플로우(overflow)</b>
문제를 항상 신경써야 한다.

지수 함수는 큰 값을 쉽게 return하기 때문에 오버플로우 문제가 쉽게 유발될 수 있다. 가령 $e^10$은 2만이 넘고,
$e^100$은 0이 40개가 넘는 어마어마한 값이며, $e^1000$까지 가면 `inf`가 return된다. 따라서 이런 큰 값끼리 나눗셈을 하면
표현할 수 없는 오버플로우 문제(4바이트(int)나 8바이트(float)로 수를 표기하는 컴퓨터가 이 이상 범위의 수를 표현하는 상황)가 발생한다.

이 문제를 해결하기 위해선 소프트맥스 함수를 개선해야 한다.
$$
\begin{align*}
y_k = \frac{\exp(a_k)}{\sum_{i=1}^{n} \exp(a_i)} 
&= \frac{C \exp(a_k)}{C \sum_{i=1}^{n} \exp(a_i)} \\
&= \frac{\exp(a_k + \log C)}{\sum_{i=1}^{n} \exp(a_i + \log C)} \\
&= \frac{\exp(a_k + C')}{\sum_{i=1}^{n} \exp(a_i + C')}
\end{align*}
$$

이 식은 임의의 수 $C$를 지수 함수 내부로 옮겨서 $logC$로 만들고 이걸 다시 $C'$으로 정의한 것이다.  
이 식이 의미하는 바는 결국 소프트맥스의 지수 함수를 계산할 때는 어떤 정수를 더하거나 빼도 결과는 바뀌지 않는다는 것이다.

여기서 $C'$에는 아무 값이나 들어가도 되지만 앞서 말한 오버플로우 문제를 해결하기 위한 목적으로 입력 신호 중 최댓값을 사용하는 게
일반적이다.

이렇게 오버플로우 문제까지 고려한 소프트맥스 함수는 아래와 같다.
```python
def softmax(x):
    c = np.max(x)
    exp_x = np.exp(x - c)
    sum_exp_x = np.sum(exp_x)
    y = exp_x / sum_exp_x
    return y
```

### 소프트맥스 함수의 특징
소프트맥스 함수의 return값은 0에서 1.0 사이의 실수뿐이다. 또한, 소프트맥스 함수 출력의 총합은 1이다.

이 덕분에 소프트맥스 함수의 출력을 '확률'로서 해석이 가능해진다.
0부터 1.0 사이의 실수뿐이므로, 100을 곱해 확룔로서 문제를 통계적으로 접근할 수 있게 된다.  

단, 소프트맥스 함수를 적용해도 각 원소의 대소 관계는 변치 않는다 걸 명심하자.  
지수 함수 자체가 단조 증가 함수($a <= b,\ f(a)<=f(b)$)이기 때문이다.

신경망을 이용한 분류에는 일반적으로 가장 큰 출력을 내는 뉴런에 해당하는 클래스로만 인식한다. 그리고 소프트맥스 함수를
적용해도 출력이 가장 큰 뉴런의 위치는 변치 않는다.

결과적으로 신경망으로 분류할 때는 출력층의 소프트맥스 함수 함수를 생략해도 된다. 현업에서도 지수 함수 계산에 드는 자원 낭비를
줄이고자 출력층 소프트맥스 함수는 생략하는 게 일반적이다.

머신러닝의 문제 풀이는 학습과 추론을 거쳐 이뤄진다. 학습 단계에서 모델을 학습하고, 추론 단계에서 앞서 학습한 모델로
미지의 데이터에 대한 추론을 수행한다. 방금 설명한 대로, 추론 단계에선 출력층의 소프트맥스 함수를 생략하나, 신경망을 학습시킬 때는 출력층에서도
소프트 맥스 함수를 사용한다.

### 출력층의 뉴런 수
출력층의 뉴런 수는 풀려는 문제에 맞게 적절한 수로 정해야 한다. <b>분류에서는 분류하고 싶은 클래스 수로 설정하는 것</b>이 일반적이다.

예를 들어 입력 이미지를 숫자 0부터 9 중 하나로 분류하는 문제라면 출력층의 뉴런을 10개로 설정한다.

그렇게 되면 출력층의 뉴런은 숫자 0부터 9까지 중 하나에 대응하게 되며 이 중 출력 값의 크기가 가장 큰 것이 신경망의 선택한 것이 된다.

## 손글씨 숫자 인식
신경망의 실전 예시로 손글씨를 분류하는 방법을 배워보자. 추론 과정을 구현한느 셈인데 추론 과정을 <b>신경망의 순전파(forward propagation)</b>
라고도 한다.

머신러닝과 마찬가지로 신경망도 두 단계를 거쳐 문제를 해결한다. 먼저 훈련 데이터를 사용해 가중치 매개변수를 학습하고, 추론 단계에서는 앞서 학습한
매개변수를 사용해 입력 데이터를 분류한다.

### MNIST 데이터셋
MNIST는 머신러닝 분야에서 아주 유명한 데이터셋으로, 간단한 실험부터 논문으로 발표되는 연구까지 다양한 곳에서 활용된다.

MNIST 데이터셋은 0부터 9까지의 숫자 이미지로 구성된다. 훈련 이미지는 총 6만장이며 시험 이미지는 1만장이다. 일반적으로 훈련 이미지들을 사용해
모델을 학습하고, 학습한 모델로 시험 이미지들을 얼마나 정확하게 분류하는지 평가한다.

```python
import sys, os
sys.path.append(os.path.join(os.path.dirname(__file__), '..')) # 부모 디렉터리의 파일을 가져올 수 있도록 설정

import numpy as np
from dataset.mnist import load_mnist
from PIL import Image


def img_show(img):
    pil_img = Image.fromarray(np.uint8(img))
    pil_img.show()

(x_train, t_train), (x_test, t_test) = load_mnist(flatten=True, normalize=False)

print(x_train.shape)

img = x_train[0]
label = t_train[0]
print(label)  # 5

print(img.shape)  # (784,)
img = img.reshape(28, 28)  # 형상을 원래 이미지의 크기로 변형
print(img.shape)  # (28, 28)

img_show(img)
```

위의 코드를 실행하면 첫 번째 훈련 이미지가 화면에 출력된다.  
`load_mnist()` 함수는 읽은 MNIST 데이터를 (훈련 이미지, 훈련 레이블), (시험 이미지, 시험 레이블) 형식으로 반환한다.  
params로 normalize,flatten,one_hot_label 세 가지를 설정할 수 있으며, 세 값 모두 bool 타입이다.

`normalize`는 입력 이미지의 픽셀값을 0.0 ~ 1.0 사이의 값으로 정규화할지를 정한다. False로 설정하면 입력 이미지의 픽셀은 원래 값 그대로
0 - 255 사이의 값을 유지한다. 

두 번째 인수인 `flatten`은 입력 이미지를 평탄하게, 1차원 배열로 만들지를 정한다. False로 설정하면 입력 이미지를 
1 * 28 * 28 크기의 3차원 배열로, True로 설정하면 784개의 원소로 구성된 1차원 배열로 저장한다.

세 번째 인수는 `one_hot_label`은 정답을 뜻하는 원소만 1(hot)이고 나머지는 모두 0인 배열이다. False면 숫자 형태 레이블을 저장하고
True이면 레이블을 원-핫 인코딩으로 저장한다.

위 코드에서 `flatten=True`로 설정했기에 이미지는 모두 1차원 배열로 저장되어 있다. 따라서 이미지를 다시 원래 형상(28*28px)로 변형해야 출력이
가능하기 때문에 `reshape()` 함수에 원하는 형상을 인수로 지정시켜 넘파이 배열 형상을 바꿀 수 있다.

또한 넘파이로 저장된 이미지 데이터를 PIL용 데이터 객체로 변환해야 하며, 이 변환은 `Image.fromarray()`가 수행한다.

### 신경망 추론 처리
이 MNIST 데이터셋을 갖고 추론을 수행하는 신경망을 구현하는 과정.  
이 신경망은 입력층 뉴런을 784개, 출력층 뉴런을 10개로 구성한다. 입력층이 784개인 이유는 이미지 크기가 28*28이기 때문이고, 출력층 뉴런이
10개인 이유는 0부터 9까지의 숫자를 구분하는 문제이기 때문이다.

은닉층은 총 2개로 첫 번째 은닉층에는 50개의 뉴런을, 두 번째 은닉층에는 100개의 뉴런을 배치할 것이다. (둘 다 임의값)

작업을 순서대로 처리할 함수인 `get_data()`, `init_network()`, `predict()`를 정의해보자.
```python
def get_data():
    (x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, flatten=True, one_hot_label=False)
    return x_test, t_test


def init_network():
    with open(os.path.dirname(__file__) + "/sample_weight.pkl", 'rb') as f:
        network = pickle.load(f)
    return network


def predict(network, x):
    W1, W2, W3 = network['W1'], network['W2'], network['W3']
    b1, b2, b3 = network['b1'], network['b2'], network['b3']

    a1 = np.dot(x, W1) + b1
    z1 = sigmoid(a1)
    a2 = np.dot(z1, W2) + b2
    z2 = sigmoid(a2)
    a3 = np.dot(z2, W3) + b3
    y = softmax(a3)

    return y
```

`init_network()` 함수에선 pickle 파일인 `sample_weight.pkl`에 저장된 '학습된 가중치 매개변수'를 읽는다. 이 파일에는 가중치와
편향 매개변수가 dict 변수로 저장되어 있다.

나머지 두 함수는 지금까지 배운 신경망 구현 함수와 크게 다르지 않다.

이 세 함수를 통해서 신경망 추론과 정확도 평가를 수행할 수 있다.
```python
x, t = get_data()
network = init_network()
accuracy_cnt = 0
for i in range(len(x)):
    y = predict(network, x[i])
    p= np.argmax(y) # 확률이 가장 높은 원소의 인덱스를 얻는다.
    if p == t[i]:
        accuracy_cnt += 1

print("Accuracy:" + str(float(accuracy_cnt) / len(x)))
```
먼저 MNIST 데이터셋을 얻고 네트워크를 생성한다. 이어서 반복문을 통해 x에 저장된 이미지 데이터를 하나씩
`predict()` 함수로 분류한다. `predict()` 함수는 각 레이블의 확률을 넘파이 배열로 반환한다.

그 후에 `np.argmax()` 함수로 배열 중 가장 원소의 인덱스를 구한다. 이게 곧 예측 결과가 된다. 마지막으로 신경망이 예측한
답변과 정답 레이블을 비교하여 맞춘 숫자(accuracy_cnt)를 세고, 이를 전체 이미지 수로 나눠 정확도를 구한 것이다.

