---
title: "신경망은 뭐지?"
date: "2026-01-01 19:20:18"
category: "딥러닝"
description: "퍼셉트론과 신경망의 차이점에 대해서 알아보고 신경망의 특징에 대해서 알아보자."
---

## 신경망
이전의 <a href="https://vorschlag-blog.vercel.app/posts/perceptron" class="plink">
  퍼셉트론
</a>을 배울 때 다층 퍼셉트론으로 복잡한 함수를 표현할 수 있다는 걸 알 수 있었다. 하지만 결국 가중치를
설정하는 작업(원하는 결과를 출력하도록 가중치 값을 적절히 정하는 작업)은 여전히 사람이 수동으로 해야 했었다.

하지만 신경망은 이를 해결해준다. 가중치 매개변수의 <b>적절한 값을 데이터로부터 자동으로 학습하는 능력</b>을 갖추고 있기 때문이다.
이 점이 신경망의 가장 큰 특징이다. 신경망이 입력 데이터를 어떻게 식별하는지까지 학습하는 글

### 퍼셉트론 VS 신경망
신경망은 퍼셉트론과 비슷한 점이 많으나 분명히 다른 점도 있다. 신경망을 그림으로 표현하면 아래와 같다.

<figure>
    <img src="/images/neuron.png" alt="신경망 구조의 예시 그림">
    <figcaption>신경망의 예시</figcaption>
</figure>

가장 왼쪽 줄을 입력층, 맨 오른쪽을 출력층이라고 부르며, 중간 줄을 은닉층이라고 한다. 은닉층의 뉴런은 입력층이나 출력층과 달리 사람 눈에
보이지 않는다. (캡슐화) 이 그림만 본다면 퍼셉트론과 큰 차이는 없어 보인다. 뉴런이 연결되는 방식은 퍼셉트론과 동일하기 때문이다.

퍼셉트론은 아래와 같은 구조를 갖추고 있다 (단층 퍼셉트론)
$x_1$과 $x_2$라는 두 신호를 받아서 y를 출력하는 퍼셉트론의 예시이다.

<figure>
    <img src="/images/node_ex.png" alt="입력이 2개인 퍼셉트론 예시 그림">
    <figcaption>입력이 2개인 퍼셉트론 예시</figcaption>
</figure>

이 퍼셉트론을 수식으로 나타내면 아래와 같다.  

$$
y = 
\begin{cases} 
    0\ (b + w_1 x_1 + w_2 x_2 \le 0) \\
    1\ (b + w_1 x_1 + w_2 x_2 > 0)
\end{cases}
$$

여기서 b는 편향으로 뉴런이 얼마나 쉽게 활성화되는지를 제어하고, $w_1,w_2$는 각 신호의 가중치로 각 신호의 영향력을 제어한다.
앞선 그림에선 b가 보이질 않는다. 여기에 편향을 표시하면 아래와 같은 그림이 된다.

<figure>
    <img src="/images/p_2.png" alt="편향을 명시한 퍼셉트론">
    <figcaption>편향을 명시한 퍼셉트론</figcaption>
</figure>

이 퍼셉트론의 동작은 $x_1,x_2,1$이라는 입력에 각각 가중치가 $w_1,w_2,b$로 주어진 상태로 각 신호에 가중치를 곱한 후 다음 뉴런에 전달된다.
다음 뉴런에서는 이 신호들의 합으로 0을 넘으면 1을 출력하고 아니면 0을 출력한다.

위 식을 함수로 나누어서 표현하면 더 간단하게 표현이 가능하다.

$$
y = h(b + w_1x_1 + w_2x_2)
$$

$$
h(x) = 
\begin{cases} 
    0\ (x \le 0) \\
    1\ (x > 0)
\end{cases}
$$

위처럼 바꾸면 입력 신호의 총합이 $h(x)$라는 함수를 거쳐 변환된 후, 그 변환값이 $y$의 출력이 되는 것으로 바뀌었다.
그 후에 $h(x)$ 함수는 입력이 0을 넘으면 1 아니면 0을 return한다.

### 활성화 함수
이렇게 $h(x)$와 같이 <b>입력 신호의 총합을 출력 신호로 변환하는 함수</b>를 활성화 함수라고 부른다.  
즉, 활성화 함수는 입력 신호의 총합이 활성화를 일으키는지 아닌지를 정하는 역할을 수행한다.

위의 식은 가중치가 곱해진 입력 신호의 총합을 계산하고, 그 합을 활성화 함수에 입력해 진짜 결과를 내는 2단계로 처리된다.
따라서 아래와 같이 2개식으로 다시 작성이 가능해진다.

$$
a=b+w_1x_1+w_2x_2
$$

$$
y=h(a)
$$

위의 식은 가중치가 달린 입력 신호와 편향의 총합을 계산한 후, 이를 $a$라고 칭한다. 그리고 $a$를 함수 $h()$에 넣어 $y$를 출력한다.

<figure>
    <img src="/images/fun.png" alt="활성화 함수로 표현한 퍼셉트론의 모습">
    <figcaption>활성화 함수까지 함께 표현한 퍼셉트론 처리 과정</figcaption>
</figure>

위의 그림과 같이 가중치 신호를 조합한 결과가 $a$라는 노드가 되었고, 활성화 함수 $h()$를 통해서 $y$라는 노드로 변환되는 과정으로 표현된다.
뉴런을 그릴 때는 하나의 원으로 그리나 활성화 함수의 기작을 명확하게 보여줄 때는 위의 사진처럼 뉴런 내부의 활성화 처리 과정을 명시하기도 한다.

## 활성화 함수의 종류
$$
h(x) = 
\begin{cases} 
    0\ (x \le 0) \\
    1\ (x > 0)
\end{cases}
$$
위와 같은 활성화 함수는 임겟값을 경계로 출력이 바뀌는데(이 경우 0), 이러한 함수를 <b>계단 함수</b>라고 한다. 따라서 퍼셉트론에서는 활성화 함수로
계단함수를 이용한다고 정의할 수도 있다. 즉, 활성화 함수로 쓸 수 있는 여러 후보 중에서 퍼셉트론은 계단 함수를 채용하고 있는 셈이다.

그렇다면 다른 활성화 함수에는 뭐가 있는 걸까?

### 시그모이드 함수
시그모이드 함수는 신경망에서 자주 이용되는 활성화 함수이다.  
$$
h(x) = \frac{1}{1 + \exp(-x)}
$$

$exp(-x)$는 $e^-x$를 의미하고, $e$는 자연상수로 2.7182의 값을 갖는 실수이다. 구조는 복잡하나 결국 함수 특정 $a$라는 입력이 주어지면
$b$를 return한다. 사실 시그모이드 함수가 중요한 이유는 신경망의 활성화 함수로 사용된다는 점에서 신경망과 퍼셉트론의 차이가 생기기 때문이다.

계단 함수의 경우 <b>경곗값</b>을 바탕으로 값이 달리지는 1차 그래프 같은 성질을 갖고 있다.
```python
def step_f(x):
    if x > 0: return 1
    else return 0
```
물론 이 함수는 x는 실수만 받아들여야 하기 때문에 넘파이 배열을 인수로 넣을 수 없다. 따라서 넘파이 배열을 지원하기 위해선 아래와 같이 수정해야 한다.
```python
def step_f(x):
    y = x > 0
    return y.astype(int)
```
넘파이 배열에 부등호 연산을 수행하면 각 원소에 부등호 연산을 수행한 `bool` 타입 배열이 생성된다.
따라서 y는 `bool` 타입의 배열이 된다. 하지만 계단 함수는 0,1만을 출력하는 `int` 타입 함수이기 때문에 `bool`을 `int`로 바꿔야 한다.
마치 자바스크립트와 같이 보통 컴퓨터에선 0은 `false`, 1은 `true`로 취급된다.

계단 함수로 그래프를 그리면 아래와 같이 표시된다.
```python
def step_f(x):
    return np.array(x > 0, dtype=int)

x = np.arange(-5.0,5.0,0.1)
y = step_f(x)
plt.plot(x,y)
# y축 범위 지정
plt.ylim(-0.1,1.1)
plt.show()
```

<figure>
    <img src="/images/stair.png" alt="계단 함수 그래프의 모습 0을 기점으로 계단 모양">
    <figcaption>사진처럼 0을 경계로 계단 모양을 볼 수 있다.</figcaption>
</figure>

시그모이드 함수는 아래와 같이 간단히 구현이 가능하다.
```python
def sigmoid(x):
    return 1 / (1 + np.exp(-x))
```
이 함수는 넘파이의 브로드캐스트덕분에 넘파이 배열과의 연산이 가능하다. 아까와 동일한 인자로 그래프를 그려보면 아래와 같다.

<figure>
    <img src="/images/sig.png" alt="시그모이드 함수 그래프(S자)의 모습">
    <figcaption>s자(sigmoid) 모양을 확인할 수 있다.</figcaption>
</figure>

### 시그모이드 함수와 계단 함수의 비교
시그모이드 함수와 계단 함수는 곡선과 직선의 차이일 것이다. 계단 함수는 0을 경계로 갑작스러운 출력의 변화가 생기나
시그모이드 함수는 매끄럽게 값이 변화하기 때문이다. 시그모이드의 이 매끈함은 신경망 학습에서 매우 중요한 역할을 수행한다.

계단 함수의 return값이 0과 1인 것에 비해 시그모이드 함수는 다양한 값을 return한다는 점도 매우 다르다. <b>연속적인 실수</b>를 제공한다.

반면 두 함수의 공통점은 결국 입력값에 비례해서 출력값이 증가한다는 점이 같다. 또한 return되는 값의 범위는 결국 0부터 1의 값이다.

## 비선형 함수
계단 함수와 시그모이드 함수의 공통점은 그 밖에도 있다. 둘 다 <b>비선형 함수</b>라는 점이다. 둘 다 1차 방정식의 그래프처럼
하나의 직선으로 이뤄진 것이 아니다.

신경망에서는 활성화 함수로 <b>비선형 함수</b>를 사용해야 한다. 왜 선형 함수는 안 되고 비선형 함수는 되는 걸까?
그 이유는 <b>선형 함수를 사용할 경우 신경망의 층을 깊게하는 의미가 없어지기 떄문</b>이다.

선형 함수의 문제점은 층을 아무리 깊게 해도 '은닉츠이 없는 네트워크'로도 똑같은 기능을 할 수 있다는 점에 있다.  
예를 들어서 $h(x) = cx$라는 선형 함수가 있을 때 이를 3층 네트워크로 만들면 $y(x)=h(h(h(x)))$와 같다. 그리고 이 계산은
$y(x)=c*c*c*x$처럼 곱셉을 3번 수행하나 결국 $y(x)=ax\ a=c^3$으로 표현이 가능하다.

즉, 은닉층이 없는 네트워크로 표한할 수 있다. 이래선 여러 층으로 구성하는 이점을 살릴 수 없다.

### ReLU 함수
시그모이드 함수는 신경망 분야에서 오랜 시간 사용했으나 최근에는 <a href="https://ko.wikipedia.org/wiki/ReLU" class="plink">ReLU</a>
함수를 주로 사용한다. ReLU 함수는 입력이 0을 넘으면 그대로 출력하고 0 이하면 0을 출력하는 함수이다.

$$
h(x) = 
\begin{cases} 
    0\ (x \le 0) \\
    x\ (x > 0)
\end{cases}
$$

수식적으로 위와 같이 표현된다. python 코드로는 `maximum()` 함수를 통해서 간단하게 구현 가능하다.
```python
def relu(x):
    return np.maximum(x,0)
```

## 다차원 배열의 계산
넘파이의 다차원 배열을 사용한 계산법을 숙달하면 신경망을 효율적으로 구현할 수 있다. 다차원 배열도 숫자의 집합이다.
```python
import numpy as np
A = np.array([1, 2, 3z 4])
print(A)
# [1234]
np.ndim(A)
# 1
A.shape
# (4,)
A.shape[ ]
# 4
```
배열의 차원은 `ndim()` 함수를 통해서 알 수 있다. `shape`라는 인스턴스는 배열의 형상을 <b>튜플</b>로 반환한다.
2차원 배열은 <b>행렬</b>이라고 부르며, 가로 방향을 행, 세로 방향을 열이라고 한다.

### 행렬의 곱
행렬의 곱은 왼쪽 행렬의 행과 오른쪽 행렬의 열을 원소별로 곱하고 그 값을 더해서 계산한다.
두 행렬의 곱은 `np.dot()`으로 계산할 수 있다.
```python
A = np.array([[1, 2, 3], [4, 5, 6]])
A.shape
# (2, 3)
B = np.array([[1, 2], [3, 4], [5, 6]])
B.shape
# (3, 2)
np.dot(A, B)
# array([[22, 28],[49, 64]])
```